Connecting to Spark

```{r connection}
library(sparklyr)
library(dplyr)
library(ggplot2)
sc <- spark_connect(master = "local")

# TRUE if spark is connected 
spark_connection_is_open(sc)
```

In this notebook we'll be using a dataset of flights in USA:

```{r cache_data}
# read flights table directly into Spark
flights_tbl <- spark_read_csv(sc, 'flights', "./data/flights/flights_sample.csv")
# object_size(flights_tbl) remember that flights_tbl is a pointer no the full table (that is in Spark)
flights_tbl <- flights_tbl %>% mutate(ARR_DELAY = as.numeric(ARR_DELAY), ARR_TIME = as.numeric(ARR_TIME), 
                                      DEP_DELAY = as.numeric(DEP_DELAY), DEP_TIME = as.numeric(DEP_TIME) )
# read airlines table into Spark
airlines_tbl <- spark_read_csv(sc, 'airlines', "./data/flights/airlines.csv",header = FALSE, columns = c("AIRLINE_ID", "AIRLINE_NAME", "AIRLINE_ALIAS", "IATA", "ICAO", "CALLSIGN", "COUNTRY", "ACTIVE"))

# read airports table into Spark
airports_tbl <- spark_read_csv(sc, 'airports', "./data/flights/airports.csv")
``` 

Let's pre-process the dataset and create the variable GAIN that represents the time that the pilot has lost or won during the flight

```{r create_gain}
flights_filtered = flights_tbl %>%
  filter(!is.na(ARR_DELAY) & !is.na(DEP_DELAY) & !is.na(DISTANCE)) %>%
  filter(DEP_DELAY > 15 & DEP_DELAY < 240) %>%
  filter(ARR_DELAY > -60 & ARR_DELAY < 360) %>%
  filter(YEAR >= 2014 & YEAR <= 2015)

fliths_sql = ft_dplyr_transformer(sc, flights_filtered) 
print(fliths_sql$param_map$statement)

# Filter records and create target variable 'gain'
model_data <- flights_filtered %>%
  left_join(airlines_tbl, by = c("UNIQUE_CARRIER" = "IATA")) %>%
  mutate(GAIN = DEP_DELAY - ARR_DELAY) %>%
  select(YEAR, MONTH, ARR_DELAY, DEP_DELAY, DISTANCE, UNIQUE_CARRIER, AIRLINE_NAME, TAIL_NUM, GAIN, COUNTRY)
```

```{r summarize_carrier}
summarize_carrier = model_data %>%
  group_by(UNIQUE_CARRIER) %>%
  summarize(airline = min(AIRLINE_NAME), gain=mean(GAIN), 
            distance=mean(DISTANCE), depdelay=mean(DEP_DELAY)) %>%
  select(airline, gain, distance, depdelay) %>%
  arrange(gain)

summarize_carrier
``` 


Can we estimate the gain (or loose) of time in flight based on variables such as dicance, departure deplay, airline id or country?

```{r linear_regression}
# Partition the data into training and validation sets
model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 43)

# Fit a linear model
ml1 <- model_partition$train %>% ml_linear_regression(GAIN ~ DISTANCE + DEP_DELAY + UNIQUE_CARRIER + COUNTRY)
#ml1_alter <- model_partition$train %>% ft_r_formula(GAIN ~ DISTANCE + DEP_DELAY + UNIQUE_CARRIER + COUNTRY) %>% ml_linear_regression()


# Summarize the linear model
summary(ml1)
#ml1_alter$summary
```


Which is the prediction for a specific flight in the validation set?


```{r predict}
library(DBI)

sdf_register( model_partition$valid, name = "model_validation")
model_validation_samples <- dbGetQuery(sc, "SELECT * FROM model_validation LIMIT 10")


ml_predict(ml1, sdf_copy_to(sc, model_validation_samples[5, ]))
```

Evaluate global performance of the model

```{r performance}

# Calculate average gains by predicted decile
model_deciles <- lapply(model_partition, function(x) {
  ml_predict(ml1, x) %>%
    mutate(decile = ntile(desc(prediction), 10)) %>%
    group_by(decile) %>%
    summarize(gain = mean(GAIN)) %>%
    select(decile, gain) %>%
    collect()
})


# Create a summary dataset for plotting
deciles <- rbind(
  data.frame(data = 'train', model_deciles$train),
  data.frame(data = 'valid', model_deciles$valid),
  make.row.names = FALSE
)

# Plot average gains by predicted decile
deciles %>%
  ggplot(aes(factor(decile), gain, fill = data)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes')

``` 



## Window Functions

dplyr supports Spark SQL window functions. Window functions are used in conjunction with mutate and filter to solve a wide range of problems. You can compare the dplyr syntax to the query it has generated by using `sql_render()`.

```{r, collapse=TRUE}
# Find the most and least delayed flight each day
bestworst <- flights_tbl %>%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %>%
  select(DEP_DELAY) %>% 
  summarize(MIN_DEP_DELAY = min(DEP_DELAY), MAX_DEP_DELAY = max(DEP_DELAY)) %>% 
  arrange(YEAR, MONTH, DAY_OF_MONTH)
dbplyr::sql_render(bestworst)
bestworst
```

```{r}
# Rank each flight within a daily
ranked <- flights_tbl %>%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %>%
  select(DEP_DELAY) %>% 
  mutate(rank = rank(desc(DEP_DELAY)))
dbplyr::sql_render(ranked)
ranked
```

## Writing Data

It is often useful to save the results of your analysis or the tables that you have generated on your Spark cluster into persistent storage. The best option in many scenarios is to write the table out to a [Parquet](https://parquet.apache.org/) file using the [spark_write_parquet](reference/sparklyr/spark_write_parquet.html) function. For example:

```{r write_parquet}
spark_write_parquet(flights_tbl, "./data/flights/parquet_flights")
```

This will write the Spark DataFrame referenced by the tbl R variable to the given HDFS path. You can use the [spark_read_parquet](reference/sparklyr/spark_read_parquet.html) function to read the same table back into a subsequent Spark session:

```{r}
flights_tbl <- spark_read_parquet(sc, "flights", "./data/flights/parquet_flights")
```

You can also write data as CSV or JSON using the [spark_write_csv](reference/sparklyr/spark_write_csv.html) and [spark_write_json](reference/sparklyr/spark_write_json.html) functions.
